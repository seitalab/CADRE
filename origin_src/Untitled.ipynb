{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksuga/.pyenv/versions/3.7.0/envs/attention/lib/python3.7/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mask(y_trn, m_trn):\n",
    "  y_pos = y_trn.sum(axis=0)\n",
    "  y_neg = ((1 - y_trn) * m_trn).sum(axis=0)\n",
    "\n",
    "  y_add = np.array([[1 if (m_trn[idx,idy] == 0) and (y_pos[idy] > y_neg[idy]) else 0 for idy in range(y_trn.shape[1])] for idx in range(y_trn.shape[0])])\n",
    "\n",
    "  y_trn = y_trn + y_add\n",
    "\n",
    "  m_trn = np.ones(m_trn.shape)\n",
    "\n",
    "  return y_trn, m_trn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bin2idx(omic_bin):\n",
    "  \"\"\" Transfer a binarized matrix into a index matrix (for input of embedding layer).\n",
    "\n",
    "  omic_bin: (num_sample, num_feature), each value in {0,1}\n",
    "  omic_idx: 0 is used for padding, and therefore meaningful index starts from 1.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  num_max_omic = omic_bin.sum(axis=1).max() # max num of mutation in a single sample\n",
    "  omic_idx = np.zeros((len(omic_bin), num_max_omic), dtype=int )\n",
    "  for idx, line in enumerate(omic_bin):\n",
    "    line = [idy+1 for idy, val in enumerate(line) if val == 1]\n",
    "    omic_idx[idx][0:len(line)] = line\n",
    "\n",
    "  return omic_idx\n",
    "\n",
    "def split_dataset(dataset, ratio=0.8):\n",
    "  \"\"\" Split the dataset according to the ratio of training/test sets.\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  dataset: dict\n",
    "    dict of lists, including omic profiles, cancer types, sensitivities, sample names\n",
    "  ratio: float\n",
    "    size(train_set)/size(train_set+test_set)\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  train_set, test_set: dict\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  num_sample = len(dataset[\"tmr\"])\n",
    "  num_train_sample = int(num_sample*ratio)\n",
    "\n",
    "  train_set = {k:dataset[k][0:num_train_sample] for k in dataset.keys()}\n",
    "  test_set = {k:dataset[k][num_train_sample:] for k in dataset.keys()}\n",
    "\n",
    "  return train_set, test_set\n",
    "\n",
    "\n",
    "def get_ptw_ids(drug_info, tgt):\n",
    "\n",
    "  id2pw = {id:pw for id,pw in zip(drug_info.index,drug_info['Target pathway'])}\n",
    "  pws = [id2pw.get(int(c),'Unknown') for c in tgt.columns]\n",
    "  pw2id = {pw:id for id,pw in enumerate(list(set(pws)))}\n",
    "  ptw_ids = [pw2id[pw] for pw in pws]\n",
    "\n",
    "  return ptw_ids\n",
    "\n",
    "def load_dataset(input_dir=\"data/input\", drug_id=-1, shuffle_feature=False):\n",
    "    tgt = pd.read_csv(os.path.join(input_dir,'gdsc.csv'),index_col=0)\n",
    "    drug_info = pd.read_csv(os.path.join(input_dir,'drug_info_gdsc.csv'),index_col=0)\n",
    "    ptw_ids = get_ptw_ids(drug_info,tgt)\n",
    "    \n",
    "    omics_data = {'mut':None, 'cnv':None, 'exp':None, 'met':None}\n",
    "    for omic in omics_data.keys():\n",
    "        omics_data[omic] = pd.read_csv(\n",
    "            os.path.join(input_dir,omic+'_'+'gdsc.csv'), index_col=0)\n",
    "    \n",
    "    common_samples = [v.index for v in omics_data.values()]\n",
    "    common_samples = list( set(tgt.index).intersection(*common_samples))\n",
    "    \n",
    "    tgt = tgt.loc[common_samples]\n",
    "    for omic in omics_data.keys():\n",
    "        omics_data[omic] = omics_data[omic].loc[common_samples]\n",
    "\n",
    "    tmr = list(tgt.index) # barcodes/names of tumors\n",
    "    msk = tgt.notnull().astype(int).values # mask of target data: 1->data available, 0->nan\n",
    "    tgt = tgt.fillna(0).astype(int).values # fill nan element of target with 0.\n",
    "\n",
    "    num_sample = len(tmr)\n",
    "    \n",
    "    omics_data_keys = list(omics_data.keys())\n",
    "    for omic in omics_data_keys:\n",
    "        omic_val = omics_data.pop(omic)\n",
    "        omic_val = omic_val.values\n",
    "        omics_data[omic+'_bin'] = omic_val\n",
    "        omics_data[omic+'_idx'] = bin2idx(omics_data[omic+'_bin'])\n",
    "    \n",
    "    \n",
    "    omics_data['tgt'] = tgt\n",
    "    omics_data['msk'] = msk\n",
    "    omics_data['tmr'] = tmr\n",
    "    \n",
    "    return omics_data, ptw_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, ptw_ids = load_dataset(input_dir='../data/input',  drug_id=-1)\n",
    "\n",
    "train_set, test_set = split_dataset(dataset, ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mut_bin', 'mut_idx', 'cnv_bin', 'cnv_idx', 'exp_bin', 'exp_idx', 'met_bin', 'met_idx', 'tgt', 'msk', 'tmr'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set['tgt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['tgt'],train_set['msk'] = fill_mask(train_set['tgt'],train_set['msk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(676, 260)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set['tgt'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tgt'],dataset['msk'] = fill_mask(dataset['tgt'],dataset['msk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 1, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 1, 0, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['tgt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
