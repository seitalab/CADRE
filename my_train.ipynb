{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksuga/.pyenv/versions/3.7.0/envs/attention/lib/python3.7/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from my_utils import Encoder , Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mask(y_trn, m_trn):\n",
    "  y_pos = y_trn.sum(axis=0)\n",
    "  y_neg = ((1 - y_trn) * m_trn).sum(axis=0)\n",
    "\n",
    "  y_add = np.array([[1 if (m_trn[idx,idy] == 0) and (y_pos[idy] > y_neg[idy]) else 0 for idy in range(y_trn.shape[1])] for idx in range(y_trn.shape[0])])\n",
    "\n",
    "  y_trn = y_trn + y_add\n",
    "\n",
    "  m_trn = np.ones(m_trn.shape)\n",
    "\n",
    "  return y_trn, m_trn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bin2idx(omic_bin):\n",
    "  \"\"\" Transfer a binarized matrix into a index matrix (for input of embedding layer).\n",
    "\n",
    "  omic_bin: (num_sample, num_feature), each value in {0,1}\n",
    "  omic_idx: 0 is used for padding, and therefore meaningful index starts from 1.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  num_max_omic = omic_bin.sum(axis=1).max() # max num of mutation in a single sample\n",
    "  omic_idx = np.zeros((len(omic_bin), num_max_omic), dtype=int )\n",
    "  for idx, line in enumerate(omic_bin):\n",
    "    line = [idy+1 for idy, val in enumerate(line) if val == 1]\n",
    "    omic_idx[idx][0:len(line)] = line\n",
    "\n",
    "  return omic_idx\n",
    "\n",
    "def get_ptw_ids(drug_info, tgt):\n",
    "\n",
    "  id2pw = {id:pw for id,pw in zip(drug_info.index,drug_info['Target pathway'])}\n",
    "  pws = [id2pw.get(int(c),'Unknown') for c in tgt.columns]\n",
    "  pw2id = {pw:id for id,pw in enumerate(list(set(pws)))}\n",
    "  ptw_ids = [pw2id[pw] for pw in pws]\n",
    "\n",
    "  return ptw_ids\n",
    "\n",
    "def load_dataset(input_dir=\"data/input\", drug_id=-1, shuffle_feature=False):\n",
    "    tgt = pd.read_csv(os.path.join(input_dir,'gdsc.csv'),index_col=0)\n",
    "    drug_info = pd.read_csv(os.path.join(input_dir,'drug_info_gdsc.csv'),index_col=0)\n",
    "    ptw_ids = get_ptw_ids(drug_info,tgt)\n",
    "    \n",
    "    omics_data = {'mut':None, 'cnv':None, 'exp':None, 'met':None}\n",
    "    for omic in omics_data.keys():\n",
    "        omics_data[omic] = pd.read_csv(\n",
    "            os.path.join(input_dir,omic+'_'+'gdsc.csv'), index_col=0)\n",
    "    \n",
    "    common_samples = [v.index for v in omics_data.values()]\n",
    "    common_samples = list( set(tgt.index).intersection(*common_samples))\n",
    "    \n",
    "    tgt = tgt.loc[common_samples]\n",
    "    for omic in omics_data.keys():\n",
    "        omics_data[omic] = omics_data[omic].loc[common_samples]\n",
    "\n",
    "    tmr = list(tgt.index) # barcodes/names of tumors\n",
    "    msk = tgt.notnull().astype(int).values # mask of target data: 1->data available, 0->nan\n",
    "    tgt = tgt.fillna(0).astype(int).values # fill nan element of target with 0.\n",
    "\n",
    "    num_sample = len(tmr)\n",
    "    \n",
    "    omics_data_keys = list(omics_data.keys())\n",
    "    for omic in omics_data_keys:\n",
    "        omic_val = omics_data.pop(omic)\n",
    "        omic_val = omic_val.values\n",
    "        omics_data[omic+'_bin'] = omic_val\n",
    "        omics_data[omic+'_idx'] = bin2idx(omics_data[omic+'_bin'])\n",
    "    \n",
    "    \n",
    "    omics_data['tgt'] = tgt\n",
    "    omics_data['msk'] = msk\n",
    "    omics_data['tmr'] = tmr\n",
    "    \n",
    "    return omics_data, ptw_ids\n",
    "\n",
    "def split_dataset(dataset, ratio=0.8):\n",
    "\n",
    "  num_sample = len(dataset[\"tmr\"])\n",
    "  num_train_sample = int(num_sample*ratio)\n",
    "\n",
    "  train_set = {k:dataset[k][0:num_train_sample] for k in dataset.keys()}\n",
    "  test_set = {k:dataset[k][num_train_sample:] for k in dataset.keys()}\n",
    "\n",
    "  return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, ptw_ids = load_dataset(input_dir='data/input', drug_id=-1)\n",
    "train_set, test_set = split_dataset(dataset, ratio=0.8)\n",
    "train_set['tgt'],train_set['msk'] = fill_mask(train_set['tgt'],train_set['msk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "omc_size =dataset['exp_bin'].shape[1]\n",
    "drg_size =dataset['tgt'].shape[1]\n",
    "emb_dim = 200\n",
    "train_size = len(train_set['tmr'])\n",
    "test_size = len(test_set['tmr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CadreDataset(data.Dataset):\n",
    "    def __init__(self,dataset_split, phase='train'):\n",
    "        \n",
    "        self.dataset = dataset_split\n",
    "        self.phase = phase\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.dataset['tmr'])\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        exp_idx = self.dataset['exp_idx'][index]\n",
    "        \n",
    "        labels = [self.dataset['tgt'][index], self.dataset['msk'][index]]\n",
    "\n",
    "        return exp_idx,labels\n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "#dataset --> trainset testset\n",
    "#dataloader --> trainloader, testloader\n",
    "# dataloaders_dict = {'Train': -----, 'test':------}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CadreDataset(train_set,phase='train')\n",
    "test_dataset = CadreDataset(test_set,phase='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "train_dataloader = data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "dataloaders_dict = {'train':train_dataloader, 'test':test_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network define \n",
    "class CadreNet(nn.Module):\n",
    "    def __init__(self,ptw_ids,drg_size,omc_size,emb_dim,device):\n",
    "        super(CadreNet, self).__init__()\n",
    "        \n",
    "        self.ptw_ids = ptw_ids\n",
    "        self.drg_size = drg_size\n",
    "        \n",
    "        self.drg_ids = np.array([list(range(self.drg_size))])\n",
    "        self.drg_ids = torch.LongTensor(self.drg_ids)\n",
    "        self.drg_ids = self.drg_ids.to(device)\n",
    "        self.ptw_ids = torch.LongTensor([self.ptw_ids])\n",
    "        self.ptw_ids = self.ptw_ids.to(device)\n",
    "        \n",
    "        \n",
    "        self.encoder = Encoder(omc_size,ptw_ids)\n",
    "        self.decoder = Decoder(emb_dim,drg_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        hid_omc= self.encoder(inputs,self.ptw_ids)\n",
    "        logit_drg=self.decoder(hid_omc,self.drg_ids)\n",
    "        \n",
    "        return logit_drg\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CadreNet(\n",
       "  (encoder): Encoder(\n",
       "    (layer_emb): Embedding(3001, 200, padding_idx=0)\n",
       "    (layer_dropout_0): Dropout(p=0.5, inplace=False)\n",
       "    (layer_w_0): Linear(in_features=200, out_features=128, bias=True)\n",
       "    (layer_beta): Linear(in_features=128, out_features=8, bias=True)\n",
       "    (layer_emb_ptw): Embedding(25, 128)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layer_emb_drg): Embedding(260, 200)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:',device)\n",
    "net = CadreNet(ptw_ids, drg_size, omc_size, emb_dim,device)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "epsilon = 1e-5\n",
    "optimizer = optim.SGD(params = net.parameters(), lr = 0.001, momentum =0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cross_entropy(lgt_drg,tgts,msks):\n",
    "    loss = torch.sum(\n",
    "        torch.mul(criterion(lgt_drg,tgts),msks)\n",
    "        )/ (torch.sum(msks)+epsilon)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def evaluate(labels,msks,preds,epsilon=1e-5):\n",
    "    \n",
    "    flat_labels = np.reshape(labels,-1)\n",
    "    flat_preds_nr = np.reshape(preds,-1)\n",
    "    flat_preds = np.reshape(np.around(preds),-1)\n",
    "    flat_msks = np.reshape(msks,-1)\n",
    "\n",
    "    flat_labels_msk = np.array([flat_labels[idx] for idx, val in enumerate(flat_msks) if val == 1])\n",
    "    flat_preds_msk = np.array([flat_preds[idx] for idx, val in enumerate(flat_msks) if val == 1])\n",
    "    flat_preds_nr_msk = np.array([flat_preds_nr[idx] for idx, val in enumerate(flat_msks) if val == 1])\n",
    "\n",
    "    accuracy = np.mean(flat_labels_msk == flat_preds_msk)\n",
    "    true_pos = np.dot(flat_labels_msk, flat_preds_msk)\n",
    "    precision = 1.0*true_pos/(flat_preds_msk.sum()+epsilon)\n",
    "    recall = 1.0*true_pos/(flat_labels_msk.sum()+epsilon)\n",
    "\n",
    "    f1score = 2*precision*recall/(precision+recall+epsilon)\n",
    "\n",
    "    # a bug fixed\n",
    "    fpr, tpr, _ = roc_curve(flat_labels_msk, flat_preds_nr_msk)\n",
    "    auc_val = auc(fpr, tpr)\n",
    "\n",
    "    return precision, recall, f1score,accuracy, auc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "        \n",
    "        for phase in ['train','test']:\n",
    "            \n",
    "                \n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "            \n",
    "            if (epoch == 0) and (phase == 'train'):\n",
    "                continue\n",
    "                \n",
    "            for inputs,labels in tqdm(dataloaders_dict[phase]):\n",
    "                inputs=inputs.to(device)\n",
    "                labels[0],labels[1] = labels[0].float().to(device) , labels[1].float().to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs = net(inputs)\n",
    "                    loss_ent = loss_cross_entropy(outputs,labels[0],labels[1])\n",
    "                    #labels[0] = tgts , labels[1] = msks\n",
    "                    loss = loss_ent\n",
    "                    preds = torch.sigmoid(outputs)\n",
    "                    \n",
    "    \n",
    "                \n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                epoch_loss += loss.item() * inputs.size(0)\n",
    "                #epoch_corrects += correct_num(labels[0],labels[1],preds)\n",
    "                preds = preds.detach().cpu().numpy()\n",
    "                labels[0],labels[1] = labels[0].detach().cpu().numpy(),labels[1].detach().cpu().numpy()\n",
    "                precision, recall, f1score, accuracy, auc = evaluate(labels[0], labels[1], preds)\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            #epoch_acc = epoch_corrects/ len(dataloaders_dict[phase].dataset)\n",
    "           \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} auc: {:.4f} f1: {:.4f}'.format(phase,epoch_loss,accuracy,auc*100.0,f1score*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1/22 [00:00<00:03,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:02<00:00,  7.85it/s]\n",
      "  0%|          | 0/85 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 5.6675 Acc: 0.5070 auc: 50.6716 f1: 51.1516\n",
      "Epoch 2/5\n",
      "-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:23<00:00,  3.63it/s]\n",
      "  5%|▍         | 1/22 [00:00<00:02,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 5.7456 Acc: 0.5038 auc: 50.5887 f1: 40.9606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:02<00:00,  7.85it/s]\n",
      "  0%|          | 0/85 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 5.5944 Acc: 0.4814 auc: 51.0515 f1: 46.7775\n",
      "Epoch 3/5\n",
      "-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:23<00:00,  3.60it/s]\n",
      "  5%|▍         | 1/22 [00:00<00:02,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 5.6153 Acc: 0.5019 auc: 52.2893 f1: 45.3582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:02<00:00,  7.82it/s]\n",
      "  0%|          | 0/85 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 5.4867 Acc: 0.4953 auc: 52.0017 f1: 46.9433\n",
      "Epoch 4/5\n",
      "-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 81/85 [00:22<00:01,  3.53it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "train_model(net,dataloaders_dict, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
