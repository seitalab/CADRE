{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksuga/.pyenv/versions/3.7.0/envs/attention/lib/python3.7/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from my_utils import Encoder , Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mask(y_trn, m_trn):\n",
    "  y_pos = y_trn.sum(axis=0)\n",
    "  y_neg = ((1 - y_trn) * m_trn).sum(axis=0)\n",
    "\n",
    "  y_add = np.array([[1 if (m_trn[idx,idy] == 0) and (y_pos[idy] > y_neg[idy]) else 0 for idy in range(y_trn.shape[1])] for idx in range(y_trn.shape[0])])\n",
    "\n",
    "  y_trn = y_trn + y_add\n",
    "\n",
    "  m_trn = np.ones(m_trn.shape)\n",
    "\n",
    "  return y_trn, m_trn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bin2idx(omic_bin):\n",
    "  \"\"\" Transfer a binarized matrix into a index matrix (for input of embedding layer).\n",
    "\n",
    "  omic_bin: (num_sample, num_feature), each value in {0,1}\n",
    "  omic_idx: 0 is used for padding, and therefore meaningful index starts from 1.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  num_max_omic = omic_bin.sum(axis=1).max() # max num of mutation in a single sample\n",
    "  omic_idx = np.zeros((len(omic_bin), num_max_omic), dtype=int )\n",
    "  for idx, line in enumerate(omic_bin):\n",
    "    line = [idy+1 for idy, val in enumerate(line) if val == 1]\n",
    "    omic_idx[idx][0:len(line)] = line\n",
    "\n",
    "  return omic_idx\n",
    "\n",
    "def get_ptw_ids(drug_info, tgt):\n",
    "\n",
    "  id2pw = {id:pw for id,pw in zip(drug_info.index,drug_info['Target pathway'])}\n",
    "  pws = [id2pw.get(int(c),'Unknown') for c in tgt.columns]\n",
    "  pw2id = {pw:id for id,pw in enumerate(list(set(pws)))}\n",
    "  ptw_ids = [pw2id[pw] for pw in pws]\n",
    "\n",
    "  return ptw_ids\n",
    "\n",
    "def load_dataset(input_dir=\"data/input\", drug_id=-1, shuffle_feature=False):\n",
    "    tgt = pd.read_csv(os.path.join(input_dir,'gdsc.csv'),index_col=0)\n",
    "    drug_info = pd.read_csv(os.path.join(input_dir,'drug_info_gdsc.csv'),index_col=0)\n",
    "    ptw_ids = get_ptw_ids(drug_info,tgt)\n",
    "    \n",
    "    omics_data = {'mut':None, 'cnv':None, 'exp':None, 'met':None}\n",
    "    for omic in omics_data.keys():\n",
    "        omics_data[omic] = pd.read_csv(\n",
    "            os.path.join(input_dir,omic+'_'+'gdsc.csv'), index_col=0)\n",
    "    \n",
    "    common_samples = [v.index for v in omics_data.values()]\n",
    "    common_samples = list( set(tgt.index).intersection(*common_samples))\n",
    "    \n",
    "    tgt = tgt.loc[common_samples]\n",
    "    for omic in omics_data.keys():\n",
    "        omics_data[omic] = omics_data[omic].loc[common_samples]\n",
    "\n",
    "    tmr = list(tgt.index) # barcodes/names of tumors\n",
    "    msk = tgt.notnull().astype(int).values # mask of target data: 1->data available, 0->nan\n",
    "    tgt = tgt.fillna(0).astype(int).values # fill nan element of target with 0.\n",
    "\n",
    "    num_sample = len(tmr)\n",
    "    \n",
    "    omics_data_keys = list(omics_data.keys())\n",
    "    for omic in omics_data_keys:\n",
    "        omic_val = omics_data.pop(omic)\n",
    "        omic_val = omic_val.values\n",
    "        omics_data[omic+'_bin'] = omic_val\n",
    "        omics_data[omic+'_idx'] = bin2idx(omics_data[omic+'_bin'])\n",
    "    \n",
    "    \n",
    "    omics_data['tgt'] = tgt\n",
    "    omics_data['msk'] = msk\n",
    "    omics_data['tmr'] = tmr\n",
    "    \n",
    "    return omics_data, ptw_ids\n",
    "\n",
    "def split_dataset(dataset, ratio=0.8):\n",
    "\n",
    "  num_sample = len(dataset[\"tmr\"])\n",
    "  num_train_sample = int(num_sample*ratio)\n",
    "\n",
    "  train_set = {k:dataset[k][0:num_train_sample] for k in dataset.keys()}\n",
    "  test_set = {k:dataset[k][num_train_sample:] for k in dataset.keys()}\n",
    "\n",
    "  return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, ptw_ids = load_dataset(input_dir='data/input', drug_id=-1)\n",
    "train_set, test_set = split_dataset(dataset, ratio=0.8)\n",
    "train_set['tgt'],train_set['msk'] = fill_mask(train_set['tgt'],train_set['msk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "omc_size =dataset['exp_bin'].shape[1]\n",
    "drg_size =dataset['tgt'].shape[1]\n",
    "emb_dim = 200\n",
    "train_size = len(train_set['tmr'])\n",
    "test_size = len(test_set['tmr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CadreDataset(data.Dataset):\n",
    "    def __init__(self,dataset_split, phase='train'):\n",
    "        \n",
    "        self.dataset = dataset_split\n",
    "        self.phase = phase\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.dataset['tmr'])\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        exp_idx = self.dataset['exp_idx'][index]\n",
    "        \n",
    "        labels = [self.dataset['tgt'][index], self.dataset['msk'][index]]\n",
    "\n",
    "        return exp_idx,labels\n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "#dataset --> trainset testset\n",
    "#dataloader --> trainloader, testloader\n",
    "# dataloaders_dict = {'Train': -----, 'test':------}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CadreDataset(train_set,phase='train')\n",
    "test_dataset = CadreDataset(test_set,phase='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "train_dataloader = data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "dataloaders_dict = {'train':train_dataloader, 'test':test_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network define \n",
    "class CadreNet(nn.Module):\n",
    "    def __init__(self,ptw_ids,drg_size,omc_size,emb_dim):\n",
    "        super(CadreNet, self).__init__()\n",
    "        \n",
    "        self.ptw_ids = ptw_ids\n",
    "        self.drg_size = drg_size\n",
    "        \n",
    "        self.drg_ids = np.array([list(range(self.drg_size))])\n",
    "        self.drg_ids = torch.LongTensor(self.drg_ids)\n",
    "        self.ptw_ids = torch.LongTensor([self.ptw_ids])\n",
    "        \n",
    "        \n",
    "        self.encoder = Encoder(omc_size,ptw_ids)\n",
    "        self.decoder = Decoder(emb_dim,drg_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        hid_omc= self.encoder(inputs,self.ptw_ids)\n",
    "        logit_drg=self.decoder(hid_omc,self.drg_ids)\n",
    "        \n",
    "        return logit_drg\n",
    "        \n",
    "    \n",
    "    \n",
    "net = CadreNet(ptw_ids, drg_size, omc_size, emb_dim)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "epsilon = 1e-5\n",
    "optimizer = optim.SGD(params = net.parameters(), lr = 0.001, momentum =0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cross_entropy(lgt_drg,tgts,msks):\n",
    "    loss = torch.sum(\n",
    "        torch.mul(criterion(lgt_drg,tgts),msks)\n",
    "        )/ (torch.sum(msks)+epsilon)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def accuracy(labels,msks,preds):\n",
    "    \n",
    "    flat_labels = np.reshape(labels,-1)\n",
    "    flat_preds_nr = np.reshape(preds,-1)\n",
    "    flat_preds = np.reshape(np.around(preds),-1)\n",
    "    flat_msks = np.reshape(msks,-1)\n",
    "\n",
    "    flat_labels_msk = np.array([flat_labels[idx] for idx, val in enumerate(flat_msks) if val == 1])\n",
    "    flat_preds_msk = np.array([flat_preds[idx] for idx, val in enumerate(flat_msks) if val == 1])\n",
    "    flat_preds_nr_msk = np.array([flat_preds_nr[idx] for idx, val in enumerate(flat_msks) if val == 1])\n",
    "\n",
    "    accuracy = np.mean(flat_labels_msk == flat_preds_msk)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "        \n",
    "        for phase in ['train','test']:\n",
    "            \n",
    "                \n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "            \n",
    "            if (epoch == 0) and (phase == 'train'):\n",
    "                continue\n",
    "                \n",
    "            for inputs,labels in tqdm(dataloaders_dict[phase]):\n",
    "                labels[0],labels[1] = labels[0].float() , labels[1].float()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs = net(inputs)\n",
    "                    loss_ent = loss_cross_entropy(outputs,labels[0],labels[1])\n",
    "                    #labels[0] = tgts , labels[1] = msks\n",
    "                    loss = loss_ent\n",
    "                    preds = torch.sigmoid(outputs)\n",
    "                    \n",
    "    \n",
    "                \n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                epoch_loss += loss.item() * inputs.size(0)\n",
    "                #epoch_corrects += correct_num(labels[0],labels[1],preds)\n",
    "                preds = preds.detach().numpy()\n",
    "                epoch_acc = accuracy(labels[0],labels[1],preds)\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            #epoch_acc = epoch_corrects/ len(dataloaders_dict[phase].dataset)\n",
    "           \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase,epoch_loss,epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:38<00:00,  1.74s/it]\n",
      "  0%|          | 0/85 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 5.4140 Acc: 0.4747\n",
      "Epoch 2/2\n",
      "-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [04:42<00:00,  3.33s/it]\n",
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 5.3876 Acc: 0.5019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:36<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 5.4030 Acc: 0.5084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "train_model(net,dataloaders_dict, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
