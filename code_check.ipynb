{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksuga/.pyenv/versions/3.7.0/envs/attention/lib/python3.7/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from utils import load_dataset, fill_mask, split_dataset, bool_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,ptw_ids = load_dataset(input_dir = \"data/input\", repository = \"gdsc\", drug_id = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(ptw_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,test_set = split_dataset(datset,ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"tgt\"],train_set['msk'] = fill_mask(train_set['tgt'],train_set['msk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(676, 260)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set['msk'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.zeros((3,4))\n",
    "b=np.ones((4,5))\n",
    "c=[a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-283-695d4d2f9c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'msk'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "train_set['msk'].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(input_dir=\"data/input\", repository=\"gdsc\", drug_id=-1, shuffle_feature=False):\n",
    "  \"\"\" Load dataset. Samples will be shuffled and all omics data and sensitivity\n",
    "  data will be in the same order of samples.\n",
    "\n",
    "  omics_data: dict\n",
    "    exp_bin, mut_bin, cnv_bin, met_bin, exp_idx, mut_idx, cnv_idx, met_idx\n",
    "    tmr, tgt, msk\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  assert repository in ['gdsc', 'ccle']\n",
    "\n",
    "  # load sensitivity data and multi-omics data\n",
    "  tgt = pd.read_csv(os.path.join(input_dir,repository+'.csv'), index_col=0)\n",
    "\n",
    "  drug_info = pd.read_csv(os.path.join(input_dir,'drug_info_'+repository+'.csv'), index_col=0)\n",
    "\n",
    "  ptw_ids = get_ptw_ids(drug_info, tgt, repository)\n",
    "\n",
    "\n",
    "  omics_data = {'mut':None, 'cnv':None, 'exp':None, 'met':None}\n",
    "  for omic in omics_data.keys():\n",
    "    omics_data[omic] = pd.read_csv(\n",
    "        os.path.join(input_dir,omic+'_'+repository+'.csv'), index_col=0)\n",
    "\n",
    "  # find samples that have all four types of omics data\n",
    "  # 846 samples for gdsc, 409 samples for ccle\n",
    "  common_samples = [v.index for v in omics_data.values()]\n",
    "  common_samples = list( set(tgt.index).intersection(*common_samples) )\n",
    "\n",
    "  tgt = tgt.loc[common_samples]\n",
    "  for omic in omics_data.keys():\n",
    "    omics_data[omic] = omics_data[omic].loc[common_samples]\n",
    "\n",
    "  tmr = list(tgt.index) # barcodes/names of tumors\n",
    "  msk = tgt.notnull().astype(int).values # mask of target data: 1->data available, 0->nan\n",
    "  tgt = tgt.fillna(0).astype(int).values # fill nan element of target with 0.\n",
    "\n",
    "  num_sample = len(tmr)\n",
    "\n",
    "  rng = []\n",
    "  with open('data/input/rng.txt', 'r') as f:\n",
    "    for line in f:\n",
    "      v = int(line.strip())\n",
    "      if v < num_sample:\n",
    "        rng.append(v)\n",
    "\n",
    "  tmr = [tmr[i] for i in rng]\n",
    "  msk = msk[rng]\n",
    "  tgt = tgt[rng]\n",
    "\n",
    "  omics_data_keys = list(omics_data.keys())\n",
    "  for omic in omics_data_keys:\n",
    "    omic_val = omics_data.pop(omic)\n",
    "    omic_val = omic_val.values\n",
    "    if shuffle_feature:\n",
    "      # shuffle features of each sample (in place)\n",
    "      for l in omic_val:\n",
    "        np.random.shuffle(l)\n",
    "    omics_data[omic+'_bin'] = omic_val\n",
    "    omics_data[omic+'_bin'] = omics_data[omic+'_bin'][rng]\n",
    "    omics_data[omic+'_idx'] = bin2idx(omics_data[omic+'_bin'])\n",
    "\n",
    "  omics_data['tgt'] = tgt\n",
    "  omics_data['msk'] = msk\n",
    "  omics_data['tmr'] = tmr\n",
    "\n",
    "  if drug_id != -1:\n",
    "    omics_data[\"tgt\"] = np.expand_dims(tgt[:,drug_id], axis=1)\n",
    "    omics_data[\"msk\"] = np.expand_dims(msk[:,drug_id], axis=1)\n",
    "\n",
    "  return omics_data, ptw_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "input_dir = 'data/input'\n",
    "tgt = pd.read_csv(os.path.join(input_dir,'gdsc.csv'), index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository ='gdsc'\n",
    "omics_data = {'mut':None, 'cnv':None, 'exp':None, 'met':None}\n",
    "for omic in omics_data.keys():\n",
    "    omics_data[omic] = pd.read_csv(\n",
    "        os.path.join(input_dir,omic+'_'+repository+'.csv'), index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_samples = [v.index for v in omics_data.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846\n"
     ]
    }
   ],
   "source": [
    "common_samples = list(set(tgt.index).intersection(*common_samples))\n",
    "print(len(common_samples))\n",
    "tgt1 = tgt.loc[common_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1026</th>\n",
       "      <th>1028</th>\n",
       "      <th>1029</th>\n",
       "      <th>1030</th>\n",
       "      <th>1031</th>\n",
       "      <th>1032</th>\n",
       "      <th>1033</th>\n",
       "      <th>1036</th>\n",
       "      <th>1037</th>\n",
       "      <th>1038</th>\n",
       "      <th>...</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>94</th>\n",
       "      <th>104</th>\n",
       "      <th>106</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>119</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>COSMIC.687514</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.906795</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.905982</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.910936</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.906852</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.949161</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.949176</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.949171</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.906808</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.1480372</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>846 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                1026  1028  1029  1030  1031  1032  1033  1036  1037  1038  \\\n",
       "COSMIC.687514    1.0   0.0   0.0   0.0   1.0   1.0   0.0   0.0   0.0   0.0   \n",
       "COSMIC.906795    1.0   0.0   0.0   1.0   1.0   0.0   1.0   1.0   1.0   1.0   \n",
       "COSMIC.905982    1.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
       "COSMIC.910936    1.0   0.0   0.0   0.0   1.0   1.0   0.0   0.0   1.0   0.0   \n",
       "COSMIC.906852    1.0   0.0   0.0   0.0   1.0   1.0   0.0   0.0   0.0   0.0   \n",
       "...              ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "COSMIC.949161    0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
       "COSMIC.949176    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "COSMIC.949171    0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
       "COSMIC.906808    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "COSMIC.1480372   0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "                ...   87   88   89   94  104  106  110  111  119  127  \n",
       "COSMIC.687514   ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "COSMIC.906795   ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "COSMIC.905982   ...  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  \n",
       "COSMIC.910936   ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "COSMIC.906852   ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "...             ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "COSMIC.949161   ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "COSMIC.949176   ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "COSMIC.949171   ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  \n",
       "COSMIC.906808   ...  1.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "COSMIC.1480372  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[846 rows x 260 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "omic_exp = pd.read_csv(os.path.join(input_dir,'exp_gdsc.csv'),index_col=0)\n",
    "sample_names = [omic_exp.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_names = list(set(tgt.index).intersection(*sample_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes!\n"
     ]
    }
   ],
   "source": [
    "if len(sample_names) == len(dataset['tmr']:\n",
    "    print('yes!')\n",
    "else:\n",
    "    print('no!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt2 = tgt.loc[sample_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1026</th>\n",
       "      <th>1028</th>\n",
       "      <th>1029</th>\n",
       "      <th>1030</th>\n",
       "      <th>1031</th>\n",
       "      <th>1032</th>\n",
       "      <th>1033</th>\n",
       "      <th>1036</th>\n",
       "      <th>1037</th>\n",
       "      <th>1038</th>\n",
       "      <th>...</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>94</th>\n",
       "      <th>104</th>\n",
       "      <th>106</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>119</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>COSMIC.687514</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.906795</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.905982</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.910936</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.906852</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.949161</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.949176</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.949171</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.906808</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COSMIC.1480372</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>846 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                1026  1028  1029  1030  1031  1032  1033  1036  1037  1038  \\\n",
       "COSMIC.687514    1.0   0.0   0.0   0.0   1.0   1.0   0.0   0.0   0.0   0.0   \n",
       "COSMIC.906795    1.0   0.0   0.0   1.0   1.0   0.0   1.0   1.0   1.0   1.0   \n",
       "COSMIC.905982    1.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
       "COSMIC.910936    1.0   0.0   0.0   0.0   1.0   1.0   0.0   0.0   1.0   0.0   \n",
       "COSMIC.906852    1.0   0.0   0.0   0.0   1.0   1.0   0.0   0.0   0.0   0.0   \n",
       "...              ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "COSMIC.949161    0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
       "COSMIC.949176    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "COSMIC.949171    0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
       "COSMIC.906808    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "COSMIC.1480372   0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "                ...   87   88   89   94  104  106  110  111  119  127  \n",
       "COSMIC.687514   ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "COSMIC.906795   ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "COSMIC.905982   ...  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  \n",
       "COSMIC.910936   ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "COSMIC.906852   ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "...             ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "COSMIC.949161   ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "COSMIC.949176   ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "COSMIC.949171   ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  \n",
       "COSMIC.906808   ...  1.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "COSMIC.1480372  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[846 rows x 260 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1026', '1028', '1029', '1030', '1031', '1032', '1033', '1036', '1037',\n",
       "       '1038',\n",
       "       ...\n",
       "       '87', '88', '89', '94', '104', '106', '110', '111', '119', '127'],\n",
       "      dtype='object', length=260)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmr = list(tgt2.index)\n",
    "msk = tgt2.notnull().astype(int).values\n",
    "tgt = tgt2.fillna(0).astype(int).values\n",
    "num_sample = len(tmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = []\n",
    "with open('data/input/rng.txt', 'r') as f:\n",
    "    for line in f:\n",
    "      v = int(line.strip())\n",
    "      if v < num_sample:\n",
    "        rng.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin2idx(omic_bin):\n",
    "  \"\"\" Transfer a binarized matrix into a index matrix (for input of embedding layer).\n",
    "\n",
    "  omic_bin: (num_sample, num_feature), each value in {0,1}\n",
    "  omic_idx: 0 is used for padding, and therefore meaningful index starts from 1.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  num_max_omic = omic_bin.sum(axis=1).max() # max num of mutation in a single sample\n",
    "  omic_idx = np.zeros((len(omic_bin), num_max_omic), dtype=int )\n",
    "  for idx, line in enumerate(omic_bin):\n",
    "    line = [idy+1 for idy, val in enumerate(line) if val == 1]\n",
    "    omic_idx[idx][0:len(line)] = line\n",
    "\n",
    "  return omic_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ptw_ids(drug_info, tgt, repository):\n",
    "\n",
    "  id2pw = {id:pw for id,pw in zip(drug_info.index,drug_info['Target pathway'])}\n",
    "\n",
    "  if repository == 'gdsc':\n",
    "    #GDSC\n",
    "    pws = [id2pw.get(int(c),'Unknown') for c in tgt.columns]\n",
    "  else:\n",
    "    #CCLE\n",
    "    pws = [id2pw.get(c,'Unknown') for c in tgt.columns]\n",
    "\n",
    "  pw2id = {pw:id for id,pw in enumerate(list(set(pws)))}\n",
    "\n",
    "  ptw_ids = [pw2id[pw] for pw in pws]\n",
    "\n",
    "  return ptw_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(input_dir=\"data/input\", repository=\"gdsc\", drug_id=-1, shuffle_feature=False):\n",
    "  \"\"\" Load dataset. Samples will be shuffled and all omics data and sensitivity\n",
    "  data will be in the same order of samples.\n",
    "\n",
    "  omics_data: dict\n",
    "    exp_bin, mut_bin, cnv_bin, met_bin, exp_idx, mut_idx, cnv_idx, met_idx\n",
    "    tmr, tgt, msk\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  assert repository in ['gdsc', 'ccle']\n",
    "\n",
    "  # load sensitivity data and multi-omics data\n",
    "  tgt = pd.read_csv(os.path.join(input_dir,repository+'.csv'), index_col=0)\n",
    "\n",
    "  drug_info = pd.read_csv(os.path.join(input_dir,'drug_info_'+repository+'.csv'), index_col=0)\n",
    "\n",
    "  ptw_ids = get_ptw_ids(drug_info, tgt, repository)\n",
    "\n",
    "\n",
    "  omics_data = {'mut':None, 'cnv':None, 'exp':None, 'met':None}\n",
    "  for omic in omics_data.keys():\n",
    "    omics_data[omic] = pd.read_csv(\n",
    "        os.path.join(input_dir,omic+'_'+repository+'.csv'), index_col=0)\n",
    "\n",
    "  # find samples that have all four types of omics data\n",
    "  # 846 samples for gdsc, 409 samples for ccle\n",
    "  common_samples = [v.index for v in omics_data.values()]\n",
    "  common_samples = list( set(tgt.index).intersection(*common_samples) )\n",
    "\n",
    "  tgt = tgt.loc[common_samples]\n",
    "  for omic in omics_data.keys():\n",
    "    omics_data[omic] = omics_data[omic].loc[common_samples]\n",
    "\n",
    "  tmr = list(tgt.index) # barcodes/names of tumors\n",
    "  msk = tgt.notnull().astype(int).values # mask of target data: 1->data available, 0->nan\n",
    "  tgt = tgt.fillna(0).astype(int).values # fill nan element of target with 0.\n",
    "\n",
    "  num_sample = len(tmr)\n",
    "\n",
    "  rng = []\n",
    "  with open('data/input/rng.txt', 'r') as f:\n",
    "    for line in f:\n",
    "      v = int(line.strip())\n",
    "      if v < num_sample:\n",
    "        rng.append(v)\n",
    "\n",
    "  tmr = [tmr[i] for i in rng]\n",
    "  msk = msk[rng]\n",
    "  tgt = tgt[rng]\n",
    "\n",
    "  omics_data_keys = list(omics_data.keys())\n",
    "  for omic in omics_data_keys:\n",
    "    omic_val = omics_data.pop(omic)\n",
    "    omic_val = omic_val.values\n",
    "    if shuffle_feature:\n",
    "      # shuffle features of each sample (in place)\n",
    "      for l in omic_val:\n",
    "        np.random.shuffle(l)\n",
    "    omics_data[omic+'_bin'] = omic_val\n",
    "    omics_data[omic+'_bin'] = omics_data[omic+'_bin'][rng]\n",
    "    omics_data[omic+'_idx'] = bin2idx(omics_data[omic+'_bin'])\n",
    "\n",
    "  omics_data['tgt'] = tgt\n",
    "  omics_data['msk'] = msk\n",
    "  omics_data['tmr'] = tmr\n",
    "\n",
    "  if drug_id != -1:\n",
    "    omics_data[\"tgt\"] = np.expand_dims(tgt[:,drug_id], axis=1)\n",
    "    omics_data[\"msk\"] = np.expand_dims(msk[:,drug_id], axis=1)\n",
    "\n",
    "  return omics_data, ptw_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics_data,ptw_ids = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mut_bin', 'mut_idx', 'cnv_bin', 'cnv_idx', 'exp_bin', 'exp_idx', 'met_bin', 'met_idx', 'tgt', 'msk', 'tmr'])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omics_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = split_dataset(omics_data, ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['tgt'], train_set['msk'] = fill_mask(train_set['tgt'], train_set['msk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_size = omics_data['exp_bin'].shape[1]\n",
    "mut_size = omics_data['mut_bin'].shape[1]\n",
    "cnv_size = omics_data['cnv_bin'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "class ExpEncoder(nn.Module):\n",
    "  \"\"\" Encoder module with/without self-attention function to encode omic information.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self, omc_size, hidden_dim, dropout_rate=0.5, embedding_dim=512,\n",
    "      use_attention=True, attention_size=400, attention_head=8, init_gene_emb=True,\n",
    "      use_cntx_attn=True, ptw_ids=None, use_hid_lyr=False, use_relu=False,\n",
    "      repository='gdsc'):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    omc_size: number of input genes whose embeddings will be trained.\n",
    "    hidden_dim: output hidden layer dimension.\n",
    "    dropout_rate: dropout rate after each hidden layer.\n",
    "    embedding_dim: embedding dimentions of genes.\n",
    "    use_attention: whether use attention mechanism or not.\n",
    "    attention_size: dimension of linear-tanh transformed embeddings.\n",
    "    attention_head: number of heads for self-attention mechanism.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    super(ExpEncoder, self).__init__()\n",
    "\n",
    "    self.use_hid_lyr = use_hid_lyr\n",
    "    self.use_relu = use_relu\n",
    "    self.repository = repository\n",
    "\n",
    "    if init_gene_emb:\n",
    "      if self.repository == 'gdsc':\n",
    "        #GDSC dataset\n",
    "        gene_emb_pretrain = np.genfromtxt('data/input/exp_emb_gdsc.csv', delimiter=',')\n",
    "      else:\n",
    "        gene_emb_pretrain = np.genfromtxt('data/input/exp_emb_ccle.csv', delimiter=',')\n",
    "\n",
    "      self.layer_emb = nn.Embedding.from_pretrained(\n",
    "          torch.FloatTensor(gene_emb_pretrain), freeze=True, padding_idx=0)\n",
    "\n",
    "    else:\n",
    "      self.layer_emb = nn.Embedding(\n",
    "          num_embeddings=omc_size+1,\n",
    "          embedding_dim=embedding_dim,\n",
    "          padding_idx=0)\n",
    "\n",
    "    self.layer_dropout_0 = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    if self.use_hid_lyr:\n",
    "      self.layer_w_1 = nn.Linear(\n",
    "          in_features=embedding_dim,\n",
    "          out_features=hidden_dim,\n",
    "          bias=True)\n",
    "\n",
    "      self.layer_dropout_1 = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    self.use_attention = use_attention\n",
    "    self.use_cntx_attn = use_cntx_attn\n",
    "    # additional self-attention is used if specified\n",
    "    if self.use_attention:\n",
    "\n",
    "      self.layer_w_0 = nn.Linear(\n",
    "          in_features=embedding_dim,\n",
    "          out_features=attention_size,\n",
    "          bias=True)\n",
    "\n",
    "      self.layer_beta = nn.Linear(\n",
    "          in_features=attention_size,\n",
    "          out_features=attention_head,\n",
    "          bias=True)\n",
    "\n",
    "      if self.use_cntx_attn:\n",
    "        self.layer_emb_ptw = nn.Embedding(\n",
    "            num_embeddings=max(ptw_ids)+1,\n",
    "            embedding_dim=attention_size)\n",
    "\n",
    "\n",
    "  def forward(self, omc_idx, ptw_ids):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    omc_idx: int array with shape (batch_size, num_omc)\n",
    "      indices of perturbed genes in the omic data of samples\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    E_t = self.layer_emb(omc_idx) #(batch_size, num_omc, embedding_dim)\n",
    "\n",
    "\n",
    "    if self.use_attention:\n",
    "\n",
    "      E_t = torch.unsqueeze(E_t,1) #(batch_size, 1, num_omc, embedding_dim)\n",
    "      E_t = E_t.repeat(1,len(ptw_ids),1,1) #(batch_size, num_drg, num_omc, embedding_dim)\n",
    "\n",
    "      if self.use_cntx_attn:\n",
    "        Ep_t = self.layer_emb_ptw(ptw_ids) #(1, num_drg, attention_size)\n",
    "        Ep_t = torch.unsqueeze(Ep_t,2) #(1, num_drg, 1, attention_size)\n",
    "        Ep_t = Ep_t.repeat(omc_idx.shape[0],1,omc_idx.shape[1],1) #(batch_size, num_drg, num_omc, attention_size)\n",
    "\n",
    "        E_t_1 = torch.tanh( self.layer_w_0(E_t) + Ep_t) #(batch_size, num_drg, num_omc, attention_size)\n",
    "\n",
    "      else:\n",
    "        E_t_1 = torch.tanh( self.layer_w_0(E_t) ) #(batch_size, num_omc, attention_size)\n",
    "\n",
    "      A_omc = self.layer_beta(E_t_1) #(batch_size, num_drg, num_omc, attention_head)\n",
    "\n",
    "      A_omc = F.softmax(A_omc, dim=2) #(batch_size, num_drg, num_omc, attention_head)\n",
    "      A_omc = torch.sum(A_omc, dim=3, keepdim=True) #(batch_size, num_drg, num_omc, 1)\n",
    "\n",
    "      #(batch_size, num_drg, 1, num_omc) * (batch_size, num_drg, num_omc, embedding_dim)\n",
    "      #=(batch_size, num_drg, 1, embedding_dim)\n",
    "\n",
    "      self.Amtr = torch.squeeze(A_omc, 3) #(batch_size, num_drg, num_omc)\n",
    "\n",
    "      emb_omc = torch.sum( torch.matmul(A_omc.permute(0,1,3,2), E_t), dim=2, keepdim=False) #(batch_size, num_drg, embedding_dim)\n",
    "      print('emb_omc',emb_omc)\n",
    "\n",
    "    else:\n",
    "\n",
    "      emb_omc = torch.mean(E_t, dim=1, keepdim=False) #(batch_size, embedding_dim)\n",
    "\n",
    "      emb_omc = torch.unsqueeze(emb_omc,1) #(batch_size, 1, embedding_dim)\n",
    "\n",
    "      emb_omc = emb_omc.repeat(1,len(ptw_ids),1) #(batch_size, num_drg, embedding_dim)\n",
    "      \n",
    "\n",
    "\n",
    "    if self.use_relu:\n",
    "      hid_omc = self.layer_dropout_0(torch.relu(emb_omc))\n",
    "    else:\n",
    "      hid_omc = self.layer_dropout_0(emb_omc)\n",
    "\n",
    "    return hid_omc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ExpEncoder(\n",
    "        omc_size=exp_size, hidden_dim=200,dropout_rate=0.5 ,\n",
    "        embedding_dim=200, use_attention=True,\n",
    "        attention_size=128, attention_head=8,\n",
    "        init_gene_emb=True, use_cntx_attn=True, ptw_ids=ptw_ids,\n",
    "        use_hid_lyr=False, use_relu=False, repository='gdsc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_dataset_cuda(dataset, use_cuda):\n",
    "  \"\"\" Wrap default numpy or list data into PyTorch variables.\n",
    "  \"\"\"\n",
    "\n",
    "  batch_dataset = {'tmr':dataset['tmr']}\n",
    "  for k in ['tgt', 'msk']:\n",
    "    if k in dataset.keys():\n",
    "      batch_dataset[k] = torch.FloatTensor(dataset[k])\n",
    "\n",
    "  for k in dataset.keys():\n",
    "    if k.endswith('_idx'):\n",
    "      batch_dataset[k] = torch.LongTensor(dataset[k])\n",
    "    elif k.endswith('_bin'):\n",
    "      batch_dataset[k] = torch.FloatTensor(dataset[k])\n",
    "\n",
    "  if use_cuda:\n",
    "    for k in batch_dataset.keys():\n",
    "      if k == 'tmr':\n",
    "        continue\n",
    "      else:\n",
    "        batch_dataset[k] = batch_dataset[k].cuda()\n",
    "\n",
    "  return batch_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(dataset, rng, index, batch_size, batch_type=\"train\", use_cuda=True):\n",
    "  \"\"\" Get a mini-batch dataset for training or test -- Multi-task/label\n",
    "  learning version here, so we can take drug reponses of a cell lines as\n",
    "  a single sample.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  dataset: dict\n",
    "    dict of lists, including SGAs, cancer types, DEGs, patient barcodes\n",
    "  rng: list of id_tmr\n",
    "  index: int\n",
    "    starting index of current mini-batch\n",
    "  batch_size: int\n",
    "  batch_type: str\n",
    "    batch strategy is slightly different for training and test\n",
    "    \"train\": will return to beginning of the queue when `index` out of range\n",
    "    \"test\": will not return to beginning of the queue when `index` out of range\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  batch_dataset: dict\n",
    "    a mini-batch of the input `dataset`.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  size_rng = len(rng)\n",
    "  if batch_type == \"train\":\n",
    "    batch_dataset = {\n",
    "        k : [ dataset[k][rng[i%size_rng]] for i in range(index, index+batch_size) ] \\\n",
    "        for k in dataset.keys()}\n",
    "  elif batch_type == \"test\":\n",
    "    batch_dataset = {\n",
    "        k : [ dataset[k][rng[i]] for i in range(index, min(index+batch_size, size_rng)) ] \\\n",
    "        for k in dataset.keys()}\n",
    "\n",
    "  batch_dataset = wrap_dataset_cuda(batch_dataset, use_cuda)\n",
    "\n",
    "  return batch_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_set = get_minibatch(train_set, rng, index=0,batch_size=4,batch_type='train',use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tmr', 'tgt', 'msk', 'mut_bin', 'mut_idx', 'cnv_bin', 'cnv_idx', 'exp_bin', 'exp_idx', 'met_bin', 'met_idx'])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_set.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1500])\n"
     ]
    }
   ],
   "source": [
    "omc_idx = batch_set['exp_idx']\n",
    "print(omc_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.long>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptw_ids = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-240-94004f93791c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mptw_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'builtin_function_or_method' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-238-e938bdae17d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhid_omc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0momc_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mptw_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/envs/attention/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-216-97a7c0d1d768>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, omc_idx, ptw_ids)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mE_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(batch_size, 1, num_omc, embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m       \u001b[0mE_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mptw_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(batch_size, num_drg, num_omc, embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cntx_attn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'builtin_function_or_method' has no len()"
     ]
    }
   ],
   "source": [
    "hid_omc = encoder(omc_idx,ptw_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ptw_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dataset['tgt'] = torch.FloatTensor(dataset['tgt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([846, 260])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_dataset['tgt'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  \"\"\" Encoder module with/without self-attention function to encode omic information.\n",
    "      Expencoder -> Encoder \n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self, omc_size, hidden_dim, dropout_rate=0.5, embedding_dim=512,\n",
    "      use_attention=True, attention_size=400, attention_head=8, init_gene_emb=True,\n",
    "      use_cntx_attn=True, ptw_ids=None, use_hid_lyr=False, use_relu=False,\n",
    "      repository='gdsc'):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    omc_size: number of input genes whose embeddings will be trained.\n",
    "    hidden_dim: output hidden layer dimension.\n",
    "    dropout_rate: dropout rate after each hidden layer.\n",
    "    embedding_dim: embedding dimentions of genes.\n",
    "    attention_size: dimension of linear-tanh transformed embeddings.\n",
    "    attention_head: number of heads for self-attention mechanism.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    super(Encoder, self).__init__()\n",
    "    \n",
    "    gene_emb_pretrain = np.genfromtxt('data/input/exp_emb_gdsc.csv', delimiter=',')\n",
    "     \n",
    "\n",
    "    self.layer_emb = nn.Embedding.from_pretrained(torch.FloatTensor(gene_emb_pretrain), freeze=True, padding_idx=0)\n",
    "\n",
    "    self.layer_dropout_0 = nn.Dropout(p=dropout_rate)\n",
    " \n",
    "    self.layer_w_0 = nn.Linear(in_features=embedding_dim,out_features=attention_size,bias=True)\n",
    "\n",
    "    self.layer_beta = nn.Linear(in_features=attention_size,out_features=attention_head,bias=True)\n",
    "      \n",
    "    self.layer_emb_ptw = nn.Embedding(num_embeddings=max(ptw_ids)+1,embedding_dim=attention_size)\n",
    "\n",
    "\n",
    "  def forward(self, omc_idx, ptw_ids):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    omc_idx: int array with shape (batch_size, num_omc)\n",
    "      indices of perturbed genes in the omic data of samples\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    E_t = self.layer_emb(omc_idx) #(batch_size, num_omc, embedding_dim)\n",
    "    E_t = torch.unsqueeze(E_t,1) #(batch_size, 1, num_omc, embedding_dim)\n",
    "    E_t = E_t.repeat(1,ptw_ids.shape[1],1,1) #(batch_size, num_drg, num_omc, embedding_dim)\n",
    "\n",
    "\n",
    "    Ep_t = self.layer_emb_ptw(ptw_ids) #(1, num_drg, attention_size)\n",
    "    Ep_t = torch.unsqueeze(Ep_t,2) #(1, num_drg, 1, attention_size)\n",
    "    Ep_t = Ep_t.repeat(omc_idx.shape[0],1,omc_idx.shape[1],1) #(batch_size, num_drg, num_omc, attention_size)\n",
    "\n",
    "    E_t_1 = torch.tanh( self.layer_w_0(E_t) + Ep_t) #(batch_size, num_drg, num_omc, attention_size)\n",
    "\n",
    "\n",
    "\n",
    "    A_omc = self.layer_beta(E_t_1) #(batch_size, num_drg, num_omc, attention_head)\n",
    "\n",
    "    A_omc = F.softmax(A_omc, dim=2) #(batch_size, num_drg, num_omc, attention_head)\n",
    "    A_omc = torch.sum(A_omc, dim=3, keepdim=True) #(batch_size, num_drg, num_omc, 1)\n",
    "\n",
    "    #(batch_size, num_drg, 1, num_omc) * (batch_size, num_drg, num_omc, embedding_dim)\n",
    "    #=(batch_size, num_drg, 1, embedding_dim)\n",
    "\n",
    "    self.Amtr = torch.squeeze(A_omc, 3) #(batch_size, num_drg, num_omc)\n",
    "\n",
    "    emb_omc = torch.sum( torch.matmul(A_omc.permute(0,1,3,2), E_t), dim=2, keepdim=False) #(batch_size, num_drg, embedding_dim)\n",
    "\n",
    "    hid_omc = self.layer_dropout_0(emb_omc)\n",
    "\n",
    "    return hid_omc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-248-7496573e7e21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0momc_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-246-6bbc01b909f1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, omc_size, hidden_dim, dropout_rate, embedding_dim, use_attention, attention_size, attention_head, init_gene_emb, use_cntx_attn, ptw_ids, use_hid_lyr, use_relu, repository)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_beta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_head\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_emb_ptw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mptw_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(omc_size = exp_size,hidden_dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.zeros((3,4))\n",
    "a_torch = torch.Tensor(a).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(a_torch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(2,3,dtype = torch.long)\n",
    "print(type(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msks=batch_set['msk']\n",
    "type(msks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-304-a6a25eac8a95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mptw_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mptw_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mptw_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "ptw_ids=None\n",
    "ptw_emb = nn.Embedding(num_embeddings=max(ptw_ids)+1,embedding_dim=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-303-8fa448f74293>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'type'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
